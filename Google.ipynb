{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scrape Google News using bs4\n",
    "'''\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from requests import get\n",
    "# from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time\n",
    "# import datetime as datetime\n",
    "import random\n",
    "# Define start and end dates\n",
    "# Define start and end dates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\").date()\n",
    "end_date = datetime.strptime(\"2022-01-04\", \"%Y-%m-%d\").date()\n",
    "\n",
    "Ticker = 'Apple'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "to_merge = pd.DataFrame({'Date': [],'Ticker': [], 'Url':[], 'headline': [], 'source': [], 'snippet': []})\n",
    "it = 0\n",
    "\n",
    "while start_date <= end_date:\n",
    "    it += 1\n",
    "    time.sleep(abs(random.gauss(0,2)))\n",
    "    Date = str(start_date).replace(\"-\",\"\")\n",
    "    URL = 'https://www.google.com/search?q='+Ticker+'+'+Date+'&source=lnms&tbm=nws'\n",
    "    print(URL)\n",
    "    try:\n",
    "        raw_html = requests.get(URL, headers=headers)\n",
    "        soup = BeautifulSoup(raw_html.text, 'lxml')\n",
    "        title_list = soup.find_all(\"a\", class_=\"l lLrAF\")\n",
    "        date_list = soup.find_all(\"span\", class_=\"f nsa fwzPFf\")\n",
    "        source_list = soup.find_all(\"span\", class_=\"xQ82C e8fRJf\")\n",
    "        snippet_list = soup.find_all(\"div\", class_=\"st\")\n",
    "        print(title_list)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # Parse elements\n",
    "    title = []\n",
    "    date = []\n",
    "    source = []\n",
    "    snippet = []\n",
    "    url = []\n",
    "    for elem in title_list:\n",
    "        valid = str(elem).replace(\"<em>\", \"\").replace(\"</em>\", \"\")[19:-4]\n",
    "        out1 = valid.split(\"=\")[1][1:-6]\n",
    "        out2 = valid.split(\"=\")[-1].split('>')[-1]\n",
    "        url.append(out1)\n",
    "        title.append(out2)\n",
    "    for elem in date_list:\n",
    "        valid = str(elem)[27:-7]\n",
    "        date.append(valid)\n",
    "    for elem in source_list:\n",
    "        valid = str(elem)[27:-7]\n",
    "        source.append(valid)\n",
    "    for elem in snippet_list:\n",
    "        valid = str(elem).replace(\"<em>\", \"\").replace(\"</em>\", \"\")[16:-10]\n",
    "        snippet.append(valid)\n",
    "    if len(title) == len(date) and len(title) == len(url) and len(title) == len(source) and len(title) == len(snippet):\n",
    "        ticker = ['AAPL']*len(title)\n",
    "        news = pd.DataFrame({'Date':date,'Ticker': ticker,'Url': url, 'headline':title, 'source': source, 'snippet': snippet})\n",
    "        to_merge = pd.concat([to_merge,news])\n",
    "    \n",
    "    # Move to next date\n",
    "    start_date += timedelta(days=1)\n",
    "    print(start_date)\n",
    "    time.sleep(1)\n",
    "\n",
    "to_merge.to_csv('csv_files/PYPL_news.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new york times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scrape NY Times using NY Times API\n",
    "'''\n",
    "from nytimesarticle import articleAPI\n",
    "import time\n",
    "import csv\n",
    "api = articleAPI('kZz45A7j3sanOFqYWO9FRFCcpk6QpOao')\n",
    "\n",
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    \n",
    "    news = []\n",
    "\n",
    "    try:\n",
    "        for i in articles['response']['docs']:\n",
    "            dic = {}\n",
    "            dic['id'] = i['_id']\n",
    "            #if i['abstract'] is not None:\n",
    "            #    dic['abstract'] = i['abstract'].encode(\"utf8\")\n",
    "            dic['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "            #dic['desk'] = i['news_desk']\n",
    "            dic['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "            #dic['section'] = i['section_name']\n",
    "            if i['snippet'] is not None:\n",
    "                dic['snippet'] = i['snippet'].encode(\"utf8\")\n",
    "                dic['source'] = i['source']\n",
    "                dic['type'] = i['type_of_material']\n",
    "                dic['url'] = i['web_url']\n",
    "                dic['word_count'] = i['word_count']\n",
    "                # locations\n",
    "                locations = []\n",
    "                for x in range(0,len(i['keywords'])):\n",
    "                    if 'glocations' in i['keywords'][x]['name']:\n",
    "                        locations.append(i['keywords'][x]['value'])\n",
    "                        dic['locations'] = locations\n",
    "                        # subject\n",
    "                        subjects = []\n",
    "                        for x in range(0,len(i['keywords'])):\n",
    "                            if 'subject' in i['keywords'][x]['name']:\n",
    "                                subjects.append(i['keywords'][x]['value'])\n",
    "                                dic['subjects'] = subjects\n",
    "                                news.append(dic)\n",
    "    except:\n",
    "        pass\n",
    "    return(news)\n",
    "\n",
    "def get_articles(date,query):\n",
    "    '''\n",
    "    This function accepts a year in string format (e.g.'1980')\n",
    "    and a query (e.g.'Amnesty International') and it will\n",
    "    return a list of parsed articles (in dictionaries)\n",
    "    for that year.\n",
    "    '''\n",
    "    all_articles = []\n",
    "\n",
    "    for i in range(0,100): #NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n",
    "        try:\n",
    "            \n",
    "            articles = api.search(q = query,\n",
    "                                  fq = {'source':['Reuters','AP', 'The New York Times']},\n",
    "                                  begin_date = date + '0101',\n",
    "                                  end_date = date + '1231',\n",
    "                                  sort='oldest',\n",
    "                                  news_desk = 'business',\n",
    "                                  subject = 'business',\n",
    "                                  glocations = 'U.S.',\n",
    "                                  page = str(i))\n",
    "            print(\"he;;p\")\n",
    "            \n",
    "            articles = parse_articles(articles)\n",
    "\n",
    "            time.sleep(1)\n",
    "            all_articles = all_articles + articles\n",
    "        except:\n",
    "\n",
    "            pass\n",
    "    return(all_articles)\n",
    "\n",
    "\n",
    "\n",
    "Ticker = ['AAPL']\n",
    "for i in range(2020,2022):\n",
    "    try:\n",
    "        # print ('Processing' + str(i) + '...')\n",
    "        Ticker_year =  get_articles(str(i),'stock market activity')\n",
    "        Ticker = Ticker + Ticker_year\n",
    "        keys = Ticker[0].keys()\n",
    "        print(Ticker_year)\n",
    "        with open('csv_files/Market1.csv', 'w') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(Ticker)\n",
    "    except :\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> GOOGLE NEWS </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install GoogleNews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "\n",
    "date = '05/03/2018'\n",
    "\n",
    "# Create a GoogleNews instance\n",
    "gn = GoogleNews()\n",
    "\n",
    "# Set the search parameters\n",
    "gn.search('AAPL apple')\n",
    "gn.set_time_range(date, date)\n",
    "# Get the results\n",
    "results = gn.results()\n",
    "\n",
    "# Extract the headlines and summaries\n",
    "headlines = [result['title'] for result in results]\n",
    "summaries = [result['desc'] for result in results]\n",
    "\n",
    "# Print the headlines and summaries\n",
    "for headline, summary in zip(headlines, summaries):\n",
    "    print(headline)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "date = '05/03/2023'\n",
    "dt = datetime.strptime(date, '%d/%m/%Y')\n",
    "\n",
    "i = 3\n",
    "while i > 0:\n",
    "    dt = dt + timedelta(days=1)\n",
    "    date = dt.strftime('%d/%m/%Y')\n",
    "    print(date)\n",
    "    i = i - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "from GoogleNews import GoogleNews\n",
    "\n",
    "tickers = [\"AAPL\"]\n",
    "days = 3\n",
    "\n",
    "# Set up GoogleNews object\n",
    "googlenews = GoogleNews(start='01/04/2023',end='today')\n",
    "\n",
    "# Set up empty DataFrame to store results\n",
    "df = pd.DataFrame(columns=[\"company\", \"date\", \"headline\", \"summary\"])\n",
    "\n",
    "# Loop over each day, fetch news for each ticker, and append to DataFrame\n",
    "date_str = '01/01/2022'\n",
    "for i in range(days):\n",
    "    date = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "    date_str = (date + timedelta(days=1)).strftime('%m/%d/%Y')\n",
    "    print(f\"Fetching news for {date_str}\")\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        googlenews.search(ticker)\n",
    "        news = googlenews.results(sort=True)\n",
    "        for j in range(min(5, len(news))):\n",
    "            headline = news[j][\"title\"]\n",
    "            summary = news[j][\"desc\"]\n",
    "            df = df.append({\"company\": ticker, \"date\": date_str, \"headline\": headline, \"summary\": summary}, ignore_index=True)\n",
    "    \n",
    "    # Append to CSV file\n",
    "    df.to_csv(\"news.csv\", mode=\"a\", header=not bool(i), index=False)\n",
    "    df = pd.DataFrame(columns=[\"company\", \"date\", \"headline\", \"summary\"])\n",
    "    \n",
    "    # Wait for 1 second before making the next request\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for 2023-01-01\n",
      "Fetching news for 2023-01-02\n",
      "Fetching news for 2023-01-03\n",
      "Fetching news for 2023-01-04\n",
      "Fetching news for 2023-01-05\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from GoogleNews import GoogleNews\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-01-05'\n",
    "\n",
    "# Convert start and end dates to datetime objects\n",
    "start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "df = pd.DataFrame(columns=['company', 'date', 'headline', 'summary'])\n",
    "\n",
    "# Loop over each day in the date range\n",
    "while start <= end:\n",
    "    # Convert the current date to string format\n",
    "    date_str = start.strftime('%Y-%m-%d')\n",
    "    print(f\"Fetching news for {date_str}\")\n",
    "    \n",
    "    # Create a GoogleNews instance\n",
    "    gn = GoogleNews()\n",
    "\n",
    "    # Set the search parameters\n",
    "    gn.search('AAPL apple')\n",
    "    gn.set_time_range(date_str, date_str)\n",
    "    \n",
    "    # Get the results\n",
    "    results = gn.results()\n",
    "    \n",
    "    # Extract the headlines and summaries\n",
    "    headlines = [result['title'] for result in results]\n",
    "    summaries = [result['desc'] for result in results]\n",
    "    \n",
    "    # Append the results to the dataframe\n",
    "    for headline, summary in zip(headlines, summaries):\n",
    "        df = df.append({\"company\": \"AAPL\", \"date\": date_str, \"headline\": headline, \"summary\": summary}, ignore_index=True)\n",
    "    \n",
    "    # Wait for 1 second to avoid being blocked by Google\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Increment the date by 1 day\n",
    "    start += timedelta(days=1)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv('news.csv', mode='a', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOGLE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-11\n",
      "2022-01-12\n",
      "2022-01-12\n",
      "2022-01-13\n",
      "2022-01-13\n",
      "2022-01-14\n",
      "2022-01-14\n",
      "2022-01-15\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from requests import get\n",
    "# from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time\n",
    "# import datetime as datetime\n",
    "import random\n",
    "# Define start and end dates\n",
    "# Define start and end dates\n",
    "from datetime import datetime, timedelta\n",
    "import bs4\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-12\n",
      "2022-01-12\n",
      "2022-01-13\n",
      "2022-01-13\n",
      "2022-01-14\n",
      "2022-01-14\n",
      "2022-01-15\n",
      "2022-01-15\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(\"2022-01-11\", \"%Y-%m-%d\").date()\n",
    "end_date = datetime.strptime(\"2022-01-14\", \"%Y-%m-%d\").date()\n",
    "\n",
    "while start_date <= end_date:\n",
    "    # time.sleep(abs(random.gauss(0,2)))\n",
    "    # Date = str(start_date).replace(\"-\",\"\")\n",
    "    # Move to next date\n",
    "    new_date = start_date + timedelta(days=1)\n",
    "    print(new_date)\n",
    "    start_date = new_date \n",
    "    print(start_date)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ask.com/web?q=AAPL+news&date=2023-01-01\n",
      "<Response [403]>\n",
      "b'<!DOCTYPE html><html lang=\\'en\\'><head><meta charset=\\'utf-8\\'><meta name=\\'viewport\\' content=\\'width=device-width,initial-scale=1\\'><meta name=\\'description\\' content=\\'px-captcha\\'><title>Access to this page has been denied</title></head><body id=\\'px-block\\'><div class=\\'px-logo\\'><img width=\\'400px\\' src=\\'https://amg-fe-app-images.s3.amazonaws.com/human/amg-captcha-block.png\\' alt=\\'Ask Media Group\\'></div><script>window._pxVid=\\'\\',window._pxUuid=\\'af9e54dc-ea7f-11ed-a599-655570496b57\\',window._pxAppId=\\'PX8ZOFP9vf\\',window._pxHostUrl=\\'https://collector-PX8ZOFP9vf.perimeterx.net\\',window._pxCustomLogo=\\'\\',window._pxJsClientSrc=\\'//client.perimeterx.net/PX8ZOFP9vf/main.min.js\\',window._pxFirstPartyEnabled=\\'false\\',window[\\'_\\'+window._pxAppId]={challenge:{context:{headerText:\\'Are you a person or a robot?\\',headerColor:\\'#4E4E4E\\',headerFontSize:\\'16px\\',messageText:\\'<p>Please don\\xe2\\x80\\x99t take this personally. <br>Bots and scripts can be remarkably lifelike!</p><p> Please click the button below & hold, and we\\xe2\\x80\\x99ll <br>be out of your way soon!</p>\\',messageColor:\\'#4E4E4E\\',messageFontSize:\\'14px\\'},translation:{default:{btn:\\'Click and Hold\\'}},view:{preset:1,width:\\'200px\\',height:\\'40px\\',fillColor:\\'#616161\\',borderColor:\\'#616161\\',borderRadius:30,borderWidth:2,css:[\"https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap\"],textFont:\\'Roboto\\',textColor:\\'#4E4E4E\\',texSize:14,fontWeight:700}}};var script=document.createElement(\\'script\\');script.src=\\'//captcha.perimeterx.net/PX8ZOFP9vf/captcha.js?a=c&u=af9e54dc-ea7f-11ed-a599-655570496b57&v=&m=0\\',document.head.appendChild(script),script.onerror=function(){(script=document.createElement(\\'script\\')).src=\\'https://captcha.px-cloud.net/PX8ZOFP9vf/captcha.js?a=c&u=af9e54dc-ea7f-11ed-a599-655570496b57&v=&m=0\\',script.onerror=window._pxDisplayErrorMessage,document.head.appendChild(script)},window._pxDisplayErrorMessage=function(){var e=document.createElement(\\'style\\');e.innerText=\"@import url(https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap);body{background-color:#fafbfc}@media (max-width:480px){body{background-color:#fff}}.px-captcha-error-container{position:fixed;height:328px;background-color:#fff;font-family:Roboto,sans-serif}.px-captcha-error-header{color:#f0f1f2;font-size:29px;margin:67px 0 33px;font-weight:500;line-height:.83;text-align:center}.px-captcha-error-message{color:#f0f1f2;font-size:18px;margin:0 0 29px;line-height:1.33;text-align:center}div.px-captcha-error-button{text-align:center;line-height:50px;width:253px;margin:auto;border-radius:25px;border:solid 1px #f0f1f2;font-size:20px;color:#f0f1f2}div.px-captcha-error-wrapper{margin:23px 0 0}div.px-captcha-error{margin:auto;text-align:center;width:400px;height:30px;font-size:12px;background-color:#fcf0f2;color:#ce0e2d}img.px-captcha-error{margin:6px 10px -2px 0}@media (min-width:620px){.px-captcha-error-container{width:528px;top:50%;left:50%;margin-top:-164px;margin-left:-264px;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (min-width:481px) and (max-width:620px){.px-captcha-error-container{width:85%;top:50%;left:50%;margin-top:-164px;margin-left:-42.5%;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (max-width:480px){.px-captcha-error-container{width:528px;top:50%;left:50%;margin-top:-164px;margin-left:-264px}}\";document.head.appendChild(e);var t=document.createElement(\\'div\\');t.className=\\'px-captcha-error-container\\';t.innerHTML=\"<div class=\\'px-captcha-error-header\\'>Before we continue...</div><div class=\\'px-captcha-error-message\\'>Press & Hold to confirm you are<br>a human (and not a bot).</div><div class=\\'px-captcha-error-button\\'>Press & Hold</div><div class=\\'px-captcha-error-wrapper\\'><div class=\\'px-captcha-error\\'><img class=\\'px-captcha-error\\' src=\\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAAQCAMAAADDGrRQAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAABFUExURUdwTNYELOEGONQILd0AONwALtwEL+AAL9MFLfkJSNQGLdMJLdQJLdQGLdQKLtYFLNcELdUGLdcBL9gFL88OLdUFLNEOLglBhT4AAAAXdFJOUwC8CqgNIRgRoAS1dWWuR4RTjzgryZpYblfkcAAAAI9JREFUGNNdj+sWhCAIhAdvqGVa1r7/oy6RZ7eaH3D4ZACBIed9wlOOMtUnSrEmZ6cHa9YAIfsbCkWrdpi/c50Bk2CO9mNLdMAu03wJA3HpEnfpxbyOg6ruyx8JJi6KNstnslp1dbPd9GnqmuYq7mmcv1zjnbQw8cV0xzkqo+fX1zkjUOO7wnrInUTxJiruC3vtBNRoQQn2AAAAAElFTkSuQmCC\\'>Please check your network connection or disable your ad-blocker.</div></div>\";document.body.appendChild(t)};</script><style>div.px-captcha-container{width:100%;top:0;left:0;margin-top:0;margin-left:0;position:relative;height:auto;border:none;box-shadow:none;background-color:#fafbfc}.px-logo{text-align:center}div#px-captcha{padding-top:40px}</style><script src=\\'\\'></script></body></html>'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the ticker symbol and date\n",
    "ticker = \"AAPL\"\n",
    "date = \"2023-01-01\"\n",
    "url = f\"https://www.ask.com/web?q={ticker}+news&date={date}\"\n",
    "print(url)\n",
    "# Send a GET request to Ask.com and get the HTML response\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "print(response)\n",
    "# Use BeautifulSoup to parse the HTML and extract the news articles\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "articles = soup.find_all(\"article\")\n",
    "print(html)\n",
    "# Loop through the articles and print the headline and summary for each one\n",
    "for article in articles:\n",
    "    headline = article.find(\"h2\", class_=\"PartialSearchResults-item-title\").text\n",
    "    summary = article.find(\"div\", class_=\"PartialSearchResults-item-abstract\").text\n",
    "    print(f\"Heading: {headline}\")\n",
    "    print(f\"Summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
